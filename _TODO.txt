(file sizes and computation times are listed using the full corpus of 2*1.25M tweeds)

Processing Chain
=================

1. build_vocab.sh
   build unique vocabulary set and count occurences
   -> here we select whether the small or the full corpus are used
   Time usage: 
   File size: ~ 20 MB
2. cut_vocab.sh
   filter out words with occurences < 5
   Time usage: 
   File size: ~ 1.5 MB
3. python3 pickle_vocab.py
   store filtered vocabulary set in binary file vocab.pkl
   Time usage: 
   File size: ~ 2.5 MB
4. python3 cooc.py
   -> here we also have to select the small or full corpus
   compute coocurence matrix and store in binary file cooc.pkl
   Time usage: 7min
   File size: ~ 800 MB

The following tasks (as tasks 3 and 4 above) are run in python 3 and require Numpy and SciPy.

1. Word embedding using GloVe
   store word embedding in binary file embeddings.pkl
   Time usage: 3h
   File size: ~ 35 MB

2. Building feature vectors from training dataset

3. Train linear classifier (logistic regression or SVM?)
   Time usage: long..

4. Classify test data

5. [optionally] classify small validation set (EXCLUDED FROM TRAINING SET!) for faster accuracy report

6. [optionally] automatcally build submission comment with current parameter setup for csv file submission


TODOs:
==========

Toolchain convenience / usability:

- Fix problems with large dataset:
  - I execute sum_duplicates periodically... is this the right way?
  - The large training dataset has unsanitized characters! UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 7072: character maps to <undefined>
- cooc matrix: truncate to nmax = 100 and use a coo_matrix with dtype=uint8_t  ->  less RAM usage (by factor of ~11)
- write bash script with parameters:
  - enable local validation -> set of 10k tweeds are excluded from word embedding and classifier training
  - set # of tweeds in training dataset
- evaluate usage of multithreading for computations -> does it use multiple cores?

Classification accuracy:

- exclude stopwords from vocabulary learning
- reduce tokens to base words or try to find words with similar character sequences in them -> existing algorithms?
