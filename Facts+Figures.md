Facts & Figures
===============

Performance:

- A complete run requires __ hours

Dataset vocabulary:
- number of unique words: 592559
- number of unique words with occurence >= 5: 101299
- remaining fraction of tokens in dictionary: 17.10%
- remaining fraction of words in filtered corpus: 98.23%
- fraction of stopwords in filtered dictionary: 0.15%
- fraction of stopword tokens in filtered corpus: 30.76%
- fraction of words containing numbers in filtered dictionary: 10.02%
- fraction of words containing numbers in filtered corpus: 2.26%
- fraction of special chars and punctionation words in filtered dictionary: 0.13%
- fraction of special chars and punctionation tokens in filtered corpus: 14.13%
- fraction of hashtag words in filtered dictionary: 8.41%
- fraction of hashtag tokens in filtered corpus: 0.60%
- fraction of `<user>` token in filtered corpus: 4.15%
- fraction of `<url>` token in filtered corpus: 1.36%

In total, if filtering for stopwords, numbers, punctuation and special tokens, we remove 52% of text!

Embedding:
- Number of Tweeds with only one token assigned: 1012 of 2500000
- Number of Tweeds with all tokens assigned: 1984218 of 2500000 (~80%)
- Average coverage of tokens: 98.24%
